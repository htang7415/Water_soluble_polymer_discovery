# Polymer χ(T) + Solubility ML Configuration
# Single source of truth for all hyperparameters

# Reproducibility seed
seed: 42

# ============================================================================
# Paths
# ============================================================================
paths:
  # Input data (existing Data/ directory)
  dft_chi_csv: "Data/OMG_DFT_COSMOC_chi.csv"
  exp_chi_csv: "Data/Experiment_chi_data.csv"
  solubility_csv: "Data/Binary_solubility.csv"

  # Processed data cache
  processed_dir: "data/processed"

  # Results output
  results_dir: "results"

  # Pretrained model checkpoint (for Stage 2)
  pretrained_dft_checkpoint: null  # Set to path after Stage 1, e.g., "results/dft_pretrain_20250118_123456/best_model.pt"

# ============================================================================
# Chemistry: Featurization parameters
# ============================================================================
chem:
  # Morgan fingerprint settings
  morgan_radius: 2
  morgan_n_bits: 2048

  # RDKit descriptors to compute
  # Full list of available descriptors: MolWt, LogP, TPSA, NumHDonors, NumHAcceptors,
  # FractionCSP3, NumAromaticRings, NumAliphaticRings, NumRotatableBonds, NumHeteroatoms,
  # FormalCharge, HeavyAtomCount, RingCount, MolLogP, MolMR (molar refractivity)
  descriptor_list:
    - "MolWt"
    - "MolLogP"
    - "TPSA"
    - "NumHDonors"
    - "NumHAcceptors"
    - "FractionCSP3"
    - "NumAromaticRings"
    - "NumAliphaticRings"
    - "NumRotatableBonds"
    - "NumHeteroatoms"
    - "FormalCharge"
    - "HeavyAtomCount"
    - "RingCount"

  # SMILES processing: replace '*' connection points with this dummy atom
  smiles_dummy_replacement: "C"

  # Cache settings
  force_recompute_features: false  # Set to true to ignore cache and recompute

# ============================================================================
# Training: General settings
# ============================================================================
training:
  # Device
  device: "cuda"  # "cuda" or "cpu"

  # Batch sizes
  batch_size_dft: 256       # Large batch for DFT data (many samples)
  batch_size_exp: 32        # Smaller batch for experimental χ (few samples)
  batch_size_sol: 64        # Medium batch for solubility

  # Number of epochs
  num_epochs_pretrain: 200   # Stage 1 (DFT pretraining)
  num_epochs_finetune: 200   # Stage 2 (multi-task fine-tuning)

  # Optimizer
  optimizer: "adamw"  # "adam" or "adamw"

  # Learning rates
  lr_pretrain: 1.0e-3    # Stage 1
  lr_finetune: 3.0e-4    # Stage 2 (typically lower)

  # Regularization
  weight_decay: 1.0e-4

  # Learning rate scheduler
  use_scheduler: true
  scheduler_type: "reduce_on_plateau"  # "step", "cosine", "reduce_on_plateau"
  scheduler_patience: 10    # For ReduceLROnPlateau
  scheduler_factor: 0.5     # For ReduceLROnPlateau
  scheduler_step_size: 30   # For StepLR

  # Early stopping
  early_stopping: true
  early_stopping_patience: 20

  # DataLoader settings
  num_workers: 4
  pin_memory: true

  # Gradient clipping
  grad_clip_norm: 1.0  # Set to null to disable

  # Checkpoint saving
  save_best_only: true
  save_interval: 10  # Save checkpoint every N epochs (if not save_best_only)

# ============================================================================
# Model: Architecture hyperparameters
# ============================================================================
model:
  # Shared encoder
  encoder_hidden_dims: [512, 256]  # List of hidden layer dimensions
  encoder_latent_dim: 128          # Output dimension z(polymer)
  encoder_dropout: 0.2             # Dropout probability in encoder
  encoder_activation: "relu"       # "relu", "elu", "leaky_relu"
  encoder_use_batchnorm: true      # Use batch normalization

  # χ(T) head (outputs A and B)
  chi_head_hidden_dim: 64
  chi_head_dropout: 0.1
  chi_head_activation: "relu"

  # Solubility head
  sol_head_hidden_dim: 64
  sol_head_dropout: 0.1
  sol_head_activation: "relu"

  # Reference temperature for χ_RT (Kelvin)
  T_ref_K: 298.0

# ============================================================================
# Loss: Multi-task loss weights
# ============================================================================
loss_weights:
  lambda_dft: 0.5   # Weight for DFT χ loss
  lambda_exp: 2.0   # Weight for experimental χ loss (upweight small dataset)
  lambda_sol: 1.0   # Weight for solubility loss

# ============================================================================
# Solubility: Classification settings
# ============================================================================
solubility:
  # Class weights for weighted BCE loss (handle class imbalance)
  class_weight_pos: 2.0  # Weight for soluble (positive) class
  class_weight_neg: 1.0  # Weight for insoluble (negative) class

  # Decision threshold for classification
  decision_threshold: 0.5

  # Stratified splitting (maintain class balance in train/val/test)
  stratify: true

# ============================================================================
# Data Splits
# ============================================================================
splits:
  # DFT and Solubility: 80/10/10 splits
  train_frac: 0.8
  val_frac: 0.1
  test_frac: 0.1

  # Random state for splitting
  split_seed: 42

# ============================================================================
# Cross-Validation: Experimental χ
# ============================================================================
cv:
  # Number of folds for experimental χ k-fold CV
  exp_chi_k_folds: 5

  # Shuffle data before splitting
  exp_chi_shuffle: true
  exp_chi_shuffle_seed: 42

# ============================================================================
# Uncertainty: MC Dropout settings
# ============================================================================
uncertainty:
  # Number of forward passes for MC Dropout
  mc_dropout_samples: 50

  # Keep dropout active during inference
  enable_mc_dropout: true

# ============================================================================
# Hyperparameter Optimization
# ============================================================================
hparam_search:
  # Optuna settings
  n_trials: 100
  timeout_hours: 24  # Max time for entire search

  # Pruning (early stopping for bad trials)
  use_pruning: true
  pruner: "median"  # "median" or "hyperband"

  # Objective metric weights
  # objective = roc_auc_val - alpha * mae_exp_cv
  alpha_mae_weight: 0.1

  # Search space (examples - implement in hparam_opt.py)
  search_space:
    encoder_latent_dim: [64, 128, 256]
    encoder_dropout: [0.1, 0.2, 0.3, 0.4]
    lr_finetune: [1.0e-4, 1.0e-3]  # Log scale
    lambda_exp: [1.0, 5.0]
    class_weight_pos: [1.0, 5.0]

# ============================================================================
# Logging and Monitoring
# ============================================================================
logging:
  # Console logging level: DEBUG, INFO, WARNING, ERROR
  console_level: "INFO"

  # File logging level
  file_level: "DEBUG"

  # Log every N batches during training
  log_interval: 10

  # Validation frequency (epochs)
  val_interval: 1

  # TensorBoard logging
  use_tensorboard: false
  tensorboard_dir: "runs"

# ============================================================================
# Plotting: Publication-quality figure settings
# ============================================================================
plotting:
  # Figure format
  save_png: true
  save_pdf: true
  dpi: 300

  # Style
  style: "seaborn-v0_8-paper"  # matplotlib style
  figure_size: [8, 6]  # [width, height] in inches
  font_size: 12

  # Color schemes
  colormap: "viridis"  # For temperature coloring
  palette: "Set2"      # For categorical plots
