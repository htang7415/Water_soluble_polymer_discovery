# Scaling Law Experiment Runner (SMILES)
# ==========================================
# SUBMISSION COMMANDS (run from /home/htang228):
#   mkdir -p logs
#   # Create tarball (excludes old results):
#   cd /mnt/htc-cephfs/fuse/root/staging/htang228/diffusion_model/_0106_2026_diffusion && \
#       tar --exclude='results_*' -czf /home/htang228/Diffusion_SMILES.tar.gz Diffusion_SMILES/
#   # Submit job:
#   condor_submit /mnt/htc-cephfs/fuse/root/staging/htang228/diffusion_model/_0106_2026_diffusion/Diffusion_SMILES/scripts/scaling_gpu.sub
#   # Or with different model size:
#   condor_submit -append "MODEL_SIZE=large" <path_to_this_file>
# ==========================================

# === EASY CONFIG ===
BASE_PATH = /mnt/htc-cephfs/fuse/root/staging/htang228/diffusion_model/_0106_2026_diffusion
METHOD = Diffusion_SMILES

# HTCondor settings
universe   = vanilla
initialdir = /home/htang228

# Model size: small, medium, large, xl
MODEL_SIZE = medium

# Logs in /home/ (HTC requirement)
log    = logs/smi_$(MODEL_SIZE)_$(Cluster)_$(Process).log
output = logs/smi_$(MODEL_SIZE)_$(Cluster)_$(Process).out
error  = logs/smi_$(MODEL_SIZE)_$(Cluster)_$(Process).err

# Resources
request_gpus   = 1
+WantGPULab    = true
+GPUJobLength  = "long"
request_cpus   = 16
request_memory = 256GB
request_disk   = 50GB
requirements   = (TARGET.CUDACapability >= 8.0) && (TARGET.GPUMemoryMb >= 20000)

# File transfer (enables more HTC nodes)
should_transfer_files   = YES
when_to_transfer_output = ON_EXIT

# Wrapper extracts tarball and launches the pipeline
executable = condor_wrapper.sh
transfer_input_files    = $(METHOD).tar.gz, condor_wrapper.sh, llm_env.tar.gz
transfer_output_files   = results_$(MODEL_SIZE).tar.gz
transfer_output_remaps  = "results_$(MODEL_SIZE).tar.gz=$(BASE_PATH)/$(METHOD)/results_$(MODEL_SIZE)_$(Cluster).tar.gz"

arguments = $(METHOD) $(MODEL_SIZE)

queue 1
